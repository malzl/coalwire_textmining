{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicolasmalz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 2.txt not found.\n",
      "File 3.txt not found.\n",
      "File 4.txt not found.\n",
      "File 5.txt not found.\n",
      "File 205.txt not found.\n",
      "File 217.txt not found.\n",
      "File 243.txt not found.\n",
      "File 313.txt not found.\n",
      "File 332.txt not found.\n",
      "File 456.txt not found.\n",
      "File 485.txt not found.\n",
      "File 486.txt not found.\n",
      "File 487.txt not found.\n",
      "File 488.txt not found.\n",
      "File 489.txt not found.\n",
      "File 490.txt not found.\n",
      "File 491.txt not found.\n",
      "File 492.txt not found.\n",
      "File 493.txt not found.\n",
      "File 494.txt not found.\n",
      "File 495.txt not found.\n",
      "File 496.txt not found.\n",
      "File 497.txt not found.\n",
      "File 498.txt not found.\n",
      "File 499.txt not found.\n"
     ]
    }
   ],
   "source": [
    "directory = 'output_texts/'\n",
    "\n",
    "texts = []\n",
    "\n",
    "\n",
    "# Loop through the file range\n",
    "for i in range(1, 500):\n",
    "    file_path = os.path.join(directory, f'{i}.txt')\n",
    "\n",
    "    # Check if the file exists to avoid errors\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    else:\n",
    "        print(f\"File {i}.txt not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicolasmalz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nicolasmalz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercasing\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    text = re.sub('coalwire', '', text)\n",
    "    text = re.sub('listupdate', '', text)\n",
    "    text = re.sub('subscription', '', text)\n",
    "    text = re.sub('toeditor', '', text)\n",
    "    \n",
    "    # Lemmatize text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to each newsletter\n",
    "cleaned_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "# Feature Extraction with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, min_df=10, ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "coal ash adani 2015 suggested tweet lobby dumping 2014 ash tweet proposed plant\n",
      "Topic 1:\n",
      "tweet 2016 import solar 2018 proposal domestic peabody trump analysis\n",
      "Topic 2:\n",
      "adani lignite suggested terminal 2015 tweet 2017 2013 permit email\n",
      "Topic 3:\n",
      "investigation 2014 eskom 2012 standard chartered world bank uk underground coal gasification dumping death\n",
      "Topic 4:\n",
      "suggested 2014 terminal suggested tweet tamil close coal import tweet 39 per cent coal industry\n",
      "Topic 5:\n",
      "tweet adani coal industry suggested tweet 2014 pakistan spill 2016 pdf coal india 2016\n",
      "Topic 6:\n",
      "adani unit eskom steel 2022 solar import 2030 2021 ash\n",
      "Topic 7:\n",
      "day suggested tweet coal gasification plant 2014 know reach agreement going quality movement 500 mw\n",
      "Topic 8:\n",
      "cc dam tweet billiton terminal 2014 bhp ash coal fired bhp billiton\n",
      "Topic 9:\n",
      "forced opposition proposed tell poor back away 2015 assumes challenge proposed mercury air south african\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of topics\n",
    "n_topics = 10  # Adjust based on experimentation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online')\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant_topic   6\n",
      "year              \n",
      "2013            18\n",
      "2014            52\n",
      "2015            53\n",
      "2016            52\n",
      "2017            52\n",
      "2018            52\n",
      "2019            52\n",
      "2020            53\n",
      "2021            52\n",
      "2022            38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have 500 newsletters\n",
    "num_newsletters = len(texts)\n",
    "\n",
    "# Generate dates starting from August 29, 2013, one per week\n",
    "start_date = '2013-08-29'\n",
    "dates = pd.date_range(start=start_date, periods=num_newsletters, freq='7D')\n",
    "\n",
    "# You already have the dominant_topics from the LDA model\n",
    "dominant_topics = np.argmax(lda.transform(tfidf_matrix), axis=1)\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "df = pd.DataFrame({'date': dates, 'dominant_topic': dominant_topics})\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Group by year and dominant topic to see trends\n",
    "trend_analysis = df.groupby(['year', 'dominant_topic']).size().unstack().fillna(0)\n",
    "print(trend_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2013    0.038835\n",
      "2014    0.039822\n",
      "2015    0.038121\n",
      "2016    0.046177\n",
      "2017    0.046940\n",
      "2018    0.046014\n",
      "2019    0.047068\n",
      "2020    0.045501\n",
      "2021    0.043501\n",
      "2022    0.048209\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate sentiment\n",
    "def calculate_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Calculate sentiment for each newsletter\n",
    "df['sentiment'] = [calculate_sentiment(text) for text in cleaned_texts]\n",
    "\n",
    "# Analyze sentiment trends\n",
    "sentiment_trends = df.groupby('year')['sentiment'].mean()\n",
    "print(sentiment_trends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "# Build a co-occurrence matrix for topics\n",
    "topic_combinations = itertools.combinations(range(n_topics), 2)\n",
    "co_occurrence_matrix = pd.DataFrame(0, index=range(n_topics), columns=range(n_topics))\n",
    "\n",
    "for combination in topic_combinations:\n",
    "    co_occurrence_matrix.at[combination] += np.sum((df['dominant_topic'] == combination[0]) & (df['dominant_topic'] == combination[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 0: [0]\n",
      "Community 1: [1]\n",
      "Community 2: [2]\n",
      "Community 3: [3]\n",
      "Community 4: [4]\n",
      "Community 5: [5]\n",
      "Community 6: [6]\n",
      "Community 7: [7]\n",
      "Community 8: [8]\n",
      "Community 9: [9]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_adjacency(co_occurrence_matrix)\n",
    "\n",
    "# Girvan-Newman method\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "\n",
    "communities_generator = girvan_newman(G)\n",
    "top_level_communities = next(communities_generator)\n",
    "\n",
    "# Convert communities to a more readable format\n",
    "communities = [list(community) for community in top_level_communities]\n",
    "\n",
    "# Print out the communities\n",
    "for i, community in enumerate(communities):\n",
    "    print(f\"Community {i}: {community}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw the network\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, cmap=plt.cm.RdYlBu, node_color=list(partition.values()))\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')\n",
    "\n",
    "plt.title(\"Topic Co-Occurrence Network\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
