{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model for NER\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set the directory\n",
    "directory = 'output_texts/'\n",
    "\n",
    "# Initialize list to hold texts\n",
    "texts = []\n",
    "\n",
    "# Loop through the file range\n",
    "for i in range(1, 550):\n",
    "    file_path = os.path.join(directory, f'{i}.txt')\n",
    "\n",
    "    # Check if the file exists to avoid errors\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub('coalwire', '', text)\n",
    "    text = re.sub('listupdate', '', text)\n",
    "    text = re.sub('subscription', '', text)\n",
    "    text = re.sub('toeditor', '', text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to each text\n",
    "cleaned_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "\n",
    "\n",
    "# Extracting named entities\n",
    "country_mentions = Counter()\n",
    "\n",
    "for doc in cleaned_texts:\n",
    "    spacy_doc = nlp(doc)\n",
    "    for ent in spacy_doc.ents:\n",
    "        if ent.label_ == \"GPE\":  # GPE stands for Geopolitical Entity, which includes countries\n",
    "            country_mentions[ent.text] += 1\n",
    "\n",
    "# Get the most common countries\n",
    "most_common_countries = country_mentions.most_common(20)\n",
    "\n",
    "# Plotting the results\n",
    "countries, counts = zip(*most_common_countries)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(countries, counts, color='skyblue')\n",
    "plt.xlabel('Country/Place')\n",
    "plt.ylabel('Number of Mentions')\n",
    "plt.title('Top 10 Geographical Entities Mentioned in Texts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 2.txt not found.\n",
      "File 3.txt not found.\n",
      "File 4.txt not found.\n",
      "File 5.txt not found.\n",
      "File 217.txt not found.\n",
      "File 243.txt not found.\n",
      "File 313.txt not found.\n",
      "File 332.txt not found.\n",
      "File 456.txt not found.\n",
      "File 515.txt not found.\n",
      "File 527.txt not found.\n",
      "File 528.txt not found.\n",
      "File 529.txt not found.\n",
      "File 530.txt not found.\n",
      "File 531.txt not found.\n",
      "File 532.txt not found.\n",
      "File 533.txt not found.\n",
      "File 534.txt not found.\n",
      "File 535.txt not found.\n",
      "File 536.txt not found.\n",
      "File 537.txt not found.\n",
      "File 538.txt not found.\n",
      "File 539.txt not found.\n",
      "File 540.txt not found.\n",
      "File 541.txt not found.\n",
      "File 542.txt not found.\n",
      "File 543.txt not found.\n",
      "File 544.txt not found.\n",
      "File 545.txt not found.\n",
      "File 546.txt not found.\n",
      "File 547.txt not found.\n",
      "File 548.txt not found.\n",
      "File 549.txt not found.\n"
     ]
    }
   ],
   "source": [
    "directory = 'output_texts/'\n",
    "\n",
    "texts = []\n",
    "\n",
    "\n",
    "# Loop through the file range\n",
    "for i in range(1, 550):\n",
    "    file_path = os.path.join(directory, f'{i}.txt')\n",
    "\n",
    "    # Check if the file exists to avoid errors\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    else:\n",
    "        print(f\"File {i}.txt not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicolasmalz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nicolasmalz/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercasing\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    text = re.sub('coalwire', '', text)\n",
    "    text = re.sub('listupdate', '', text)\n",
    "    text = re.sub('subscription', '', text)\n",
    "    text = re.sub('toeditor', '', text)\n",
    "    \n",
    "    # Lemmatize text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to each newsletter\n",
    "cleaned_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "# Feature Extraction with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, min_df=10, ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "tweet import suggested tweet 2014 2015 bhp billiton suggested billiton please material think included\n",
      "Topic 1:\n",
      "2015 causing chairman climate change 2009 bulletin boundary dam bhp billiton divestment abuse\n",
      "Topic 2:\n",
      "terminal recently duke energy plant 2038 met coal suggested tweet 600 million surface water long provincial government\n",
      "Topic 3:\n",
      "tweet suggested tweet suggested port terminal think march peabody divert land\n",
      "Topic 4:\n",
      "solar port 2016 renewables adani 2017 corruption 2018 chinese 2015\n",
      "Topic 5:\n",
      "unit adani 2024 steel 2023 transition solar renewables lignite sector\n",
      "Topic 6:\n",
      "tweet greenpeace fossil fuel unsubscribe preference ohio coal nuclear divestment send material think coal related arch coal california\n",
      "Topic 7:\n",
      "greenpeace 2016 terminal water port tweet eskom coal export adani air\n",
      "Topic 8:\n",
      "tweet 2014 2015 adani bhp billiton forest bloomberg billiton china coal included suggestion feature\n",
      "Topic 9:\n",
      "benefit allocate investment energy major victory report detail pursuing council energy australia mw maasvlakte\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of topics\n",
    "n_topics = 10  # Adjust based on experimentation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online')\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant_topic     0     5\n",
      "year                      \n",
      "2013             0.0  18.0\n",
      "2014             0.0  52.0\n",
      "2015             0.0  53.0\n",
      "2016             3.0  49.0\n",
      "2017            28.0  24.0\n",
      "2018             0.0  52.0\n",
      "2019             0.0  52.0\n",
      "2020             0.0  53.0\n",
      "2021             0.0  52.0\n",
      "2022             0.0  52.0\n",
      "2023             0.0  28.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have 500 newsletters\n",
    "num_newsletters = len(texts)\n",
    "\n",
    "# Generate dates starting from August 29, 2013, one per week\n",
    "start_date = '2013-08-29'\n",
    "dates = pd.date_range(start=start_date, periods=num_newsletters, freq='7D')\n",
    "\n",
    "# You already have the dominant_topics from the LDA model\n",
    "dominant_topics = np.argmax(lda.transform(tfidf_matrix), axis=1)\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "df = pd.DataFrame({'date': dates, 'dominant_topic': dominant_topics})\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Group by year and dominant topic to see trends\n",
    "trend_analysis = df.groupby(['year', 'dominant_topic']).size().unstack().fillna(0)\n",
    "print(trend_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Function to calculate sentiment\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_sentiment\u001b[39m(text):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate sentiment\n",
    "def calculate_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Calculate sentiment for each newsletter\n",
    "df['sentiment'] = [calculate_sentiment(text) for text in cleaned_texts]\n",
    "\n",
    "# Analyze sentiment trends\n",
    "sentiment_trends = df.groupby('year')['sentiment'].mean()\n",
    "print(sentiment_trends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "# Build a co-occurrence matrix for topics\n",
    "topic_combinations = itertools.combinations(range(n_topics), 2)\n",
    "co_occurrence_matrix = pd.DataFrame(0, index=range(n_topics), columns=range(n_topics))\n",
    "\n",
    "for combination in topic_combinations:\n",
    "    co_occurrence_matrix.at[combination] += np.sum((df['dominant_topic'] == combination[0]) & (df['dominant_topic'] == combination[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 0: [0]\n",
      "Community 1: [1]\n",
      "Community 2: [2]\n",
      "Community 3: [3]\n",
      "Community 4: [4]\n",
      "Community 5: [5]\n",
      "Community 6: [6]\n",
      "Community 7: [7]\n",
      "Community 8: [8]\n",
      "Community 9: [9]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_adjacency(co_occurrence_matrix)\n",
    "\n",
    "# Girvan-Newman method\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "\n",
    "communities_generator = girvan_newman(G)\n",
    "top_level_communities = next(communities_generator)\n",
    "\n",
    "# Convert communities to a more readable format\n",
    "communities = [list(community) for community in top_level_communities]\n",
    "\n",
    "# Print out the communities\n",
    "for i, community in enumerate(communities):\n",
    "    print(f\"Community {i}: {community}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw the network\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, cmap=plt.cm.RdYlBu, node_color=list(partition.values()))\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')\n",
    "\n",
    "plt.title(\"Topic Co-Occurrence Network\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
